{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification based on Logistic Regression using non-linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_data1 = 'assignment_09_data1.txt'\n",
    "fname_data2 = 'assignment_09_data2.txt'\n",
    "\n",
    "data1 = np.genfromtxt(fname_data1, delimiter=',')\n",
    "data2 = np.genfromtxt(fname_data2, delimiter=',')\n",
    "\n",
    "num_data1 = data1.shape[0]\n",
    "num_data2 = data2.shape[0]\n",
    "\n",
    "x1 = np.zeros(num_data1)\n",
    "y1 = np.zeros(num_data1)\n",
    "label1   = np.zeros(num_data1)\n",
    "\n",
    "for i in range(num_data1):\n",
    "    x1[i]  = data1[i,0]\n",
    "    y1[i]  = data1[i,1]\n",
    "    label1[i]    = data1[i,2]\n",
    "    \n",
    "    \n",
    "x2 = np.zeros(num_data2)\n",
    "y2 = np.zeros(num_data2)\n",
    "label2   = np.zeros(num_data2)\n",
    "\n",
    "for i in range(num_data2):\n",
    "    x2[i]  = data2[i,0]\n",
    "    y2[i]  = data2[i,1]\n",
    "    label2[i]    = data2[i,2]\n",
    "    \n",
    "xy_data1 = np.vstack((x1, y1)).T\n",
    "xy_data2 = np.vstack((x2, y2)).T\n",
    "\n",
    "# data[:,0] : x\n",
    "# data[:,1] : y\n",
    "# data[:,2] : label {0, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the feature function for each data to obtain the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_function1(x, y):     \n",
    "    feature = np.array([1, x, y, x*x, x*y, y*y], dtype = object)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "def feature_function2(x, y):     \n",
    "    feature = np.array([1, x, y, sin(x)])\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define regression function with a vector $\\theta$ model parameters and input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_function(theta, feature):\n",
    "    value = np.matmul(np.transpose(theta), feature)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define sigmoid function with input $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(x):\n",
    "#     for _ in range(len(x)) :\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss function with feature and label based on the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_feature(theta, feature, label):\n",
    "    z = regression_function(theta, feature)\n",
    "    h = logistic_function(z)\n",
    "    loss = (-label * np.log(h) - (1 - label) * np.log(1 - h)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define gradient vector for the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_feature(theta, feature, label):\n",
    "    z = regression_function(theta, feature)\n",
    "    h = logistic_function(z)\n",
    "    X = feature\n",
    "    gradient = np.dot(X.T, (h - label)) \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(theta, feature, label):\n",
    "    correctNum = 0\n",
    "    for i in range(0, len(feature))  :\n",
    "        z = regression_function(theta, feature[i])\n",
    "        h = logistic_function(z)\n",
    "        if label[i] == 0 :\n",
    "            if h<0.5 :\n",
    "                correctNum += 1\n",
    "        elif label[i] == 1:\n",
    "            if h>=0.5 :\n",
    "                correctNum += 1\n",
    "    accuracy = correctNum / len(label)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent for the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_iteration   = 3000         # USE THIS VALUE for the number of gradient descent iterations \n",
    "learning_rate   = 0.3           # USE THIS VALUE for the learning rate\n",
    "theta1           = np.array((0, 0, 0, 0, 0, 0))   # USE THIS VALUE for the initial condition of the model parameters\n",
    "theta2           = np.array((0, 0, 0, 0))\n",
    "theta1_iteration = np.zeros((num_iteration, theta1.size))\n",
    "loss1_iteration  = np.zeros(num_iteration)\n",
    "theta2_iteration = np.zeros((num_iteration, theta2.size))\n",
    "loss2_iteration  = np.zeros(num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1000 is different from 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f36bd59026a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeat2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_function2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtheta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcompute_gradient_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtheta1_iteration\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5b7a9334f99c>\u001b[0m in \u001b[0;36mcompute_gradient_feature\u001b[1;34m(theta, feature, label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_gradient_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregression_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8792a704d6cd>\u001b[0m in \u001b[0;36mregression_function\u001b[1;34m(theta, feature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregression_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1000 is different from 6)"
     ]
    }
   ],
   "source": [
    "feat1 = np.array([feature_function1(x1[i], y1[i]) for i in range(0, len(x1))])\n",
    "feat2 = np.array([feature_function2(x2[i], y2[i]) for i in range(0, len(x1))])\n",
    "for i in range(num_iteration):\n",
    "    theta1 = theta1 - learning_rate * compute_gradient_feature(theta1, feat1, label1)\n",
    "    loss1 = compute_loss_feature(theta1, feat1, label1)\n",
    "    theta1_iteration[i] = theta1\n",
    "    loss1_iteration[i] = loss1\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss1))\n",
    "print(\"###########################################################\")\n",
    "for i in range(num_iteration):\n",
    "    theta2 = theta2 - learning_rate * compute_gradient_feature(theta2, feat2, label2)\n",
    "    loss2 = compute_loss_feature(theta2, feat2, label2)\n",
    "    theta2_iteration[i] = theta2\n",
    "    loss2_iteration[i] = loss2\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss2))\n",
    "    \n",
    "theta1_optimal = theta1\n",
    "theta2_optimal = theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute accuracy of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = np.array([feature_function1(x1[i], y1[i]) for i in range(0, len(x1))])\n",
    "feature2 = np.array([feature_function2(x2[i], y2[i]) for i in range(0, len(x2))])\n",
    "accuracy_classifier1 = compute_accuracy(theta1_optimal, feature1, label1)\n",
    "accuracy_classifier2 = compute_accuracy(theta2_optimal, feature2, label2)\n",
    "print(accuracy_classifier1)\n",
    "print(accuracy_classifier2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))   \n",
    "    plt.title('loss')\n",
    "\n",
    "    plt.plot(loss_iteration, '-', color = 'red')\n",
    "\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(point_x, point_y, label):\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.title('data')\n",
    "    xx = []\n",
    "    yy = []\n",
    "    xxx = []\n",
    "    yyy = []\n",
    "    for i in range(0, num_data1) :\n",
    "        x = point_x[i]\n",
    "        y = point_y[i]\n",
    "        if label[i] == 0 :\n",
    "            xx.append(x)\n",
    "            yy.append(y)\n",
    "        elif label[i] == 1 :\n",
    "            xxx.append(x)\n",
    "            yyy.append(y)\n",
    "    plt.scatter(xx,yy,c='blue', label = 'Class = 0')\n",
    "    plt.scatter(xxx,yyy,c='red', label = 'Class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_parameter(theta_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('model parameter')\n",
    "    thetaT = theta_iteration.T\n",
    "    plt.plot(thetaT[0], '-', color = 'red', label = 'theta0')\n",
    "    plt.plot(thetaT[1], '-', color = 'green', label = 'theta1')\n",
    "    plt.plot(thetaT[2], '-', color = 'blue', label = 'theta2')\n",
    "    plt.plot(thetaT[3], '-', color = 'black', label = 'theta3')\n",
    "    if len(thetaT) > 4 :\n",
    "        plt.plot(thetaT[4], '-', color = 'orange', label = 'theta4')\n",
    "        plt.plot(thetaT[5], '-', color = 'skyblue', label = 'theta5')\n",
    "\n",
    "    plt.xlabel('iteration')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss_curve(loss1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss2_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta2_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier1(data, theta):\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    xx = []\n",
    "    yy = []\n",
    "    xxx = []\n",
    "    yyy = []\n",
    "    X = np.linspace(x1.min(), x1.max(), 1000)\n",
    "    Y = np.linspace(y1.min(), y1.max(), 1000)\n",
    "    gX, gY = np.meshgrid(X, Y)\n",
    "    gX = gX[0]\n",
    "    gY = gY[0]\n",
    "    Z = regression_function(theta1, feature_function1(gX, gY))\n",
    "\n",
    "    for i in range(0, num_data1) :\n",
    "        x = x1[i]\n",
    "        y = y1[i]\n",
    "        if label1[i] == 0 :\n",
    "            xx.append(x)\n",
    "            yy.append(y)\n",
    "        elif label1[i] == 1 :\n",
    "            xxx.append(x)\n",
    "            yyy.append(y)\n",
    "    print(Z.shape)\n",
    "    plt.scatter(xx, yy, color='b', label='Class = 0')\n",
    "    plt.scatter(xxx, yyy, color='r', label='Class = 1')\n",
    "    plt.contourf(gX, gY, Z, levels=100, cmap = 'bwr')\n",
    "    plt.colorbar()\n",
    "    plt.contour(gX,gY, Z, levels = [0], colors = 'black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier2(data, theta):\n",
    "    \n",
    "    plt.figure(figsize=(8,8)) # USE THIS VALUE for the size of the figure\n",
    "    #\n",
    "    # \n",
    "    # fill up the function body\n",
    "    #\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier1(data1, theta1_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(data2, theta2_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 01. plot the input data (data1) from the file [assignment_09_data1.txt] in blue for class 0 and in red for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x1,y1,label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 02. plot the input data (data2) from the file [assignment_09_data2.txt] in blue for class 0 and in red for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x2,y2,label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 03. plot the values of the model parameters $\\theta$ as curves over the gradient descent iterations using different colors for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 04. plot the values of the model parameters $\\theta$ as curves over the gradient descent iterations using different colors for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta2_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 05. plot the loss values in red curve over the gradient descent iterations for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 06. plot the loss values in red curve over the gradient descent iterations for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss2_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 07. plot the classifier with the given data points superimposed for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier1(data1, theta1_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 08. plot the classifier with the given data points superimposed for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(data2, theta2_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 09. print out the accuracy of the obtained classifier1 for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_classifier1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 10. print out the accuracy of the obtained classifier2 for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_classifier2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
