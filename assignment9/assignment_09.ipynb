{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification based on Logistic Regression using non-linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_data1 = 'assignment_09_data1.txt'\n",
    "fname_data2 = 'assignment_09_data2.txt'\n",
    "\n",
    "data1 = np.genfromtxt(fname_data1, delimiter=',')\n",
    "data2 = np.genfromtxt(fname_data2, delimiter=',')\n",
    "\n",
    "num_data1 = data1.shape[0]\n",
    "num_data2 = data2.shape[0]\n",
    "\n",
    "x1 = np.zeros(num_data1)\n",
    "y1 = np.zeros(num_data1)\n",
    "label1   = np.zeros(num_data1)\n",
    "\n",
    "for i in range(num_data1):\n",
    "    x1[i]  = data1[i,0]\n",
    "    y1[i]  = data1[i,1]\n",
    "    label1[i]    = data1[i,2]\n",
    "    \n",
    "    \n",
    "x2 = np.zeros(num_data2)\n",
    "y2 = np.zeros(num_data2)\n",
    "label2   = np.zeros(num_data2)\n",
    "\n",
    "for i in range(num_data2):\n",
    "    x2[i]  = data2[i,0]\n",
    "    y2[i]  = data2[i,1]\n",
    "    label2[i]    = data2[i,2]\n",
    "    \n",
    "xy_data1 = np.vstack((x1, y1)).T\n",
    "xy_data2 = np.vstack((x2, y2)).T\n",
    "\n",
    "# data[:,0] : x\n",
    "# data[:,1] : y\n",
    "# data[:,2] : label {0, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the feature function for each data to obtain the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_function1(x, y):     \n",
    "    feature = np.array([1, x, y, x*x, x*y, y*y], dtype = object)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "def feature_function2(x, y):     \n",
    "    feature = np.array([1, x, y, sin(x)])\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define regression function with a vector $\\theta$ model parameters and input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_function(theta, feature):\n",
    "    value = np.matmul(np.transpose(theta), feature)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define sigmoid function with input $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(x):\n",
    "#     for _ in range(len(x)) :\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss function with feature and label based on the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_feature(theta, feature, label):\n",
    "    z = regression_function(theta, feature)\n",
    "    h = logistic_function(z)\n",
    "    loss = (-label * np.log(h) - (1 - label) * np.log(1 - h)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define gradient vector for the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_feature(theta, feature, label):\n",
    "    z = regression_function(theta, feature)\n",
    "    h = logistic_function(z)\n",
    "    X = feature\n",
    "    gradient = np.dot(X.T, (h - label)) \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(theta, feature, label):\n",
    "    correctNum = 0\n",
    "    for i in range(0, len(feature))  :\n",
    "        z = regression_function(theta, feature[i])\n",
    "        h = logistic_function(z)\n",
    "        if label[i] == 0 :\n",
    "            if h<0.5 :\n",
    "                correctNum += 1\n",
    "        elif label[i] == 1:\n",
    "            if h>=0.5 :\n",
    "                correctNum += 1\n",
    "    accuracy = correctNum / len(label)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent for the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_iteration   = 3000         # USE THIS VALUE for the number of gradient descent iterations \n",
    "learning_rate   = 0.3           # USE THIS VALUE for the learning rate\n",
    "theta1           = np.array((0, 0, 0, 0, 0, 0))   # USE THIS VALUE for the initial condition of the model parameters\n",
    "theta2           = np.array((0, 0, 0, 0))\n",
    "theta1_iteration = np.zeros((num_iteration, theta1.size))\n",
    "loss1_iteration  = np.zeros(num_iteration)\n",
    "theta2_iteration = np.zeros((num_iteration, theta2.size))\n",
    "loss2_iteration  = np.zeros(num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =    0, loss = 0.53065\n",
      "iteration =    1, loss = 0.42816\n",
      "iteration =    2, loss = 0.30404\n",
      "iteration =    3, loss = 0.21912\n",
      "iteration =    4, loss = 0.15067\n",
      "iteration =    5, loss = 0.24598\n",
      "iteration =    6, loss = 0.09004\n",
      "iteration =    7, loss = 0.14333\n",
      "iteration =    8, loss = 0.12194\n",
      "iteration =    9, loss = 0.20014\n",
      "iteration =   10, loss = 0.12012\n",
      "iteration =   11, loss = 0.06714\n",
      "iteration =   12, loss = 0.06980\n",
      "iteration =   13, loss = 0.08984\n",
      "iteration =   14, loss = 0.11767\n",
      "iteration =   15, loss = 0.12684\n",
      "iteration =   16, loss = 0.04264\n",
      "iteration =   17, loss = 0.04256\n",
      "iteration =   18, loss = 0.03250\n",
      "iteration =   19, loss = 0.04995\n",
      "iteration =   20, loss = 0.02806\n",
      "iteration =   21, loss = 0.12377\n",
      "iteration =   22, loss = 0.02763\n",
      "iteration =   23, loss = 0.04194\n",
      "iteration =   24, loss = 0.07749\n",
      "iteration =   25, loss = 0.01317\n",
      "iteration =   26, loss = 0.02966\n",
      "iteration =   27, loss = 0.03873\n",
      "iteration =   28, loss = 0.10215\n",
      "iteration =   29, loss = 0.01094\n",
      "iteration =   30, loss = 0.03629\n",
      "iteration =   31, loss = 0.03618\n",
      "iteration =   32, loss = 0.06954\n",
      "iteration =   33, loss = 0.02448\n",
      "iteration =   34, loss = 0.01361\n",
      "iteration =   35, loss = 0.02989\n",
      "iteration =   36, loss = 0.03224\n",
      "iteration =   37, loss = 0.01898\n",
      "iteration =   38, loss = 0.02694\n",
      "iteration =   39, loss = 0.02503\n",
      "iteration =   40, loss = 0.00716\n",
      "iteration =   41, loss = 0.01506\n",
      "iteration =   42, loss = 0.03000\n",
      "iteration =   43, loss = 0.01641\n",
      "iteration =   44, loss = 0.01995\n",
      "iteration =   45, loss = 0.01693\n",
      "iteration =   46, loss = 0.03236\n",
      "iteration =   47, loss = 0.02439\n",
      "iteration =   48, loss = 0.01816\n",
      "iteration =   49, loss = 0.07332\n",
      "iteration =   50, loss = 0.01097\n",
      "iteration =   51, loss = 0.00844\n",
      "iteration =   52, loss = 0.01559\n",
      "iteration =   53, loss = 0.02111\n",
      "iteration =   54, loss = 0.00365\n",
      "iteration =   55, loss = 0.01453\n",
      "iteration =   56, loss = 0.00921\n",
      "iteration =   57, loss = 0.02575\n",
      "iteration =   58, loss = 0.00510\n",
      "iteration =   59, loss = 0.03738\n",
      "iteration =   60, loss = 0.01365\n",
      "iteration =   61, loss = 0.01200\n",
      "iteration =   62, loss = 0.01357\n",
      "iteration =   63, loss = 0.00558\n",
      "iteration =   64, loss = 0.00751\n",
      "iteration =   65, loss = 0.01652\n",
      "iteration =   66, loss = 0.01823\n",
      "iteration =   67, loss = 0.01711\n",
      "iteration =   68, loss = 0.00301\n",
      "iteration =   69, loss = 0.00527\n",
      "iteration =   70, loss = 0.00283\n",
      "iteration =   71, loss = 0.00728\n",
      "iteration =   72, loss = 0.00218\n",
      "iteration =   73, loss = 0.01797\n",
      "iteration =   74, loss = 0.00456\n",
      "iteration =   75, loss = 0.00188\n",
      "iteration =   76, loss = 0.00645\n",
      "iteration =   77, loss = 0.00336\n",
      "iteration =   78, loss = 0.00881\n",
      "iteration =   79, loss = 0.00658\n",
      "iteration =   80, loss = 0.00219\n",
      "iteration =   81, loss = 0.00466\n",
      "iteration =   82, loss = 0.00125\n",
      "iteration =   83, loss = 0.00177\n",
      "iteration =   84, loss = 0.00222\n",
      "iteration =   85, loss = 0.00789\n",
      "iteration =   86, loss = 0.02601\n",
      "iteration =   87, loss = 0.00637\n",
      "iteration =   88, loss = 0.00264\n",
      "iteration =   89, loss = 0.00304\n",
      "iteration =   90, loss = 0.00178\n",
      "iteration =   91, loss = 0.00267\n",
      "iteration =   92, loss = 0.00304\n",
      "iteration =   93, loss = 0.01758\n",
      "iteration =   94, loss = 0.00529\n",
      "iteration =   95, loss = 0.01482\n",
      "iteration =   96, loss = 0.00165\n",
      "iteration =   97, loss = 0.00248\n",
      "iteration =   98, loss = 0.00119\n",
      "iteration =   99, loss = 0.00382\n",
      "iteration =  100, loss = 0.00602\n",
      "iteration =  101, loss = 0.00192\n",
      "iteration =  102, loss = 0.00226\n",
      "iteration =  103, loss = 0.00243\n",
      "iteration =  104, loss = 0.00458\n",
      "iteration =  105, loss = 0.01485\n",
      "iteration =  106, loss = 0.00107\n",
      "iteration =  107, loss = 0.01256\n",
      "iteration =  108, loss = 0.00122\n",
      "iteration =  109, loss = 0.00448\n",
      "iteration =  110, loss = 0.00676\n",
      "iteration =  111, loss = 0.01092\n",
      "iteration =  112, loss = 0.00777\n",
      "iteration =  113, loss = 0.00173\n",
      "iteration =  114, loss = 0.00993\n",
      "iteration =  115, loss = 0.00104\n",
      "iteration =  116, loss = 0.00050\n",
      "iteration =  117, loss = 0.00316\n",
      "iteration =  118, loss = 0.00523\n",
      "iteration =  119, loss = 0.00832\n",
      "iteration =  120, loss = 0.00274\n",
      "iteration =  121, loss = 0.00392\n",
      "iteration =  122, loss = 0.00818\n",
      "iteration =  123, loss = 0.00487\n",
      "iteration =  124, loss = 0.00220\n",
      "iteration =  125, loss = 0.00139\n",
      "iteration =  126, loss = 0.00478\n",
      "iteration =  127, loss = 0.00464\n",
      "iteration =  128, loss = 0.00350\n",
      "iteration =  129, loss = 0.00161\n",
      "iteration =  130, loss = 0.00539\n",
      "iteration =  131, loss = 0.00266\n",
      "iteration =  132, loss = 0.00176\n",
      "iteration =  133, loss = 0.00216\n",
      "iteration =  134, loss = 0.00184\n",
      "iteration =  135, loss = 0.00347\n",
      "iteration =  136, loss = 0.00158\n",
      "iteration =  137, loss = 0.00075\n",
      "iteration =  138, loss = 0.00503\n",
      "iteration =  139, loss = 0.00133\n",
      "iteration =  140, loss = 0.00060\n",
      "iteration =  141, loss = 0.00778\n",
      "iteration =  142, loss = 0.00597\n",
      "iteration =  143, loss = 0.00732\n",
      "iteration =  144, loss = 0.00256\n",
      "iteration =  145, loss = 0.01181\n",
      "iteration =  146, loss = 0.00680\n",
      "iteration =  147, loss = 0.00732\n",
      "iteration =  148, loss = 0.00491\n",
      "iteration =  149, loss = 0.00908\n",
      "iteration =  150, loss = 0.00150\n",
      "iteration =  151, loss = 0.00182\n",
      "iteration =  152, loss = 0.00538\n",
      "iteration =  153, loss = 0.00730\n",
      "iteration =  154, loss = 0.00319\n",
      "iteration =  155, loss = 0.01039\n",
      "iteration =  156, loss = 0.00389\n",
      "iteration =  157, loss = 0.00172\n",
      "iteration =  158, loss = 0.00424\n",
      "iteration =  159, loss = 0.00674\n",
      "iteration =  160, loss = 0.00229\n",
      "iteration =  161, loss = 0.00605\n",
      "iteration =  162, loss = 0.01672\n",
      "iteration =  163, loss = 0.00161\n",
      "iteration =  164, loss = 0.01460\n",
      "iteration =  165, loss = 0.00309\n",
      "iteration =  166, loss = 0.00308\n",
      "iteration =  167, loss = 0.00253\n",
      "iteration =  168, loss = 0.00718\n",
      "iteration =  169, loss = 0.00494\n",
      "iteration =  170, loss = 0.00517\n",
      "iteration =  171, loss = 0.01548\n",
      "iteration =  172, loss = 0.00960\n",
      "iteration =  173, loss = 0.00369\n",
      "iteration =  174, loss = 0.00668\n",
      "iteration =  175, loss = 0.01087\n",
      "iteration =  176, loss = 0.02066\n",
      "iteration =  177, loss = 0.00588\n",
      "iteration =  178, loss = 0.00503\n",
      "iteration =  179, loss = 0.00705\n",
      "iteration =  180, loss = 0.01790\n",
      "iteration =  181, loss = 0.00559\n",
      "iteration =  182, loss = 0.00756\n",
      "iteration =  183, loss = 0.00230\n",
      "iteration =  184, loss = 0.03035\n",
      "iteration =  185, loss = 0.00419\n",
      "iteration =  186, loss = 0.03182\n",
      "iteration =  187, loss = 0.00688\n",
      "iteration =  188, loss = 0.00937\n",
      "iteration =  189, loss = 0.00342\n",
      "iteration =  190, loss = 0.00814\n",
      "iteration =  191, loss = 0.01498\n",
      "iteration =  192, loss = 0.02116\n",
      "iteration =  193, loss = 0.00662\n",
      "iteration =  194, loss = 0.01548\n",
      "iteration =  195, loss = 0.03917\n",
      "iteration =  196, loss = 0.02951\n",
      "iteration =  197, loss = 0.01762\n",
      "iteration =  198, loss = 0.00960\n",
      "iteration =  199, loss = 0.05390\n",
      "iteration =  200, loss = 0.02197\n",
      "iteration =  201, loss = 0.01520\n",
      "iteration =  202, loss = 0.01470\n",
      "iteration =  203, loss = 0.02499\n",
      "iteration =  204, loss = 0.01639\n",
      "iteration =  205, loss = 0.02655\n",
      "iteration =  206, loss = 0.01555\n",
      "iteration =  207, loss = 0.02087\n",
      "iteration =  208, loss = 0.00869\n",
      "iteration =  209, loss = 0.02343\n",
      "iteration =  210, loss = 0.02354\n",
      "iteration =  211, loss = 0.02097\n",
      "iteration =  212, loss = 0.03283\n",
      "iteration =  213, loss = 0.02381\n",
      "iteration =  214, loss = 0.04823\n",
      "iteration =  215, loss = 0.02366\n",
      "iteration =  216, loss = 0.04364\n",
      "iteration =  217, loss = 0.04089\n",
      "iteration =  218, loss = 0.04703\n",
      "iteration =  219, loss = 0.01916\n",
      "iteration =  220, loss = 0.06084\n",
      "iteration =  221, loss = 0.01861\n",
      "iteration =  222, loss = 0.01749\n",
      "iteration =  223, loss = 0.02147\n",
      "iteration =  224, loss = 0.04367\n",
      "iteration =  225, loss = 0.03866\n",
      "iteration =  226, loss = 0.02427\n",
      "iteration =  227, loss = 0.01367\n",
      "iteration =  228, loss = 0.07175\n",
      "iteration =  229, loss = 0.07149\n",
      "iteration =  230, loss = 0.02218\n",
      "iteration =  231, loss = 0.03132\n",
      "iteration =  232, loss = 0.04505\n",
      "iteration =  233, loss = 0.03364\n",
      "iteration =  234, loss = 0.02535\n",
      "iteration =  235, loss = 0.01669\n",
      "iteration =  236, loss = 0.07003\n",
      "iteration =  237, loss = 0.04763\n",
      "iteration =  238, loss = 0.05803\n",
      "iteration =  239, loss = 0.04143\n",
      "iteration =  240, loss = 0.03538\n",
      "iteration =  241, loss = 0.04851\n",
      "iteration =  242, loss = 0.03358\n",
      "iteration =  243, loss = 0.04868\n",
      "iteration =  244, loss = 0.02047\n",
      "iteration =  245, loss = 0.02368\n",
      "iteration =  246, loss = 0.01351\n",
      "iteration =  247, loss = 0.03268\n",
      "iteration =  248, loss = 0.05961\n",
      "iteration =  249, loss = 0.06875\n",
      "iteration =  250, loss = 0.05719\n",
      "iteration =  251, loss = 0.06081\n",
      "iteration =  252, loss = 0.04461\n",
      "iteration =  253, loss = 0.04772\n",
      "iteration =  254, loss = 0.08148\n",
      "iteration =  255, loss = 0.02542\n",
      "iteration =  256, loss = 0.05295\n",
      "iteration =  257, loss = 0.04625\n",
      "iteration =  258, loss = 0.04171\n",
      "iteration =  259, loss = 0.04018\n",
      "iteration =  260, loss = 0.02067\n",
      "iteration =  261, loss = 0.01175\n",
      "iteration =  262, loss = 0.04946\n",
      "iteration =  263, loss = 0.02876\n",
      "iteration =  264, loss = 0.05674\n",
      "iteration =  265, loss = 0.03304\n",
      "iteration =  266, loss = 0.01816\n",
      "iteration =  267, loss = 0.02585\n",
      "iteration =  268, loss = 0.02236\n",
      "iteration =  269, loss = 0.02654\n",
      "iteration =  270, loss = 0.02164\n",
      "iteration =  271, loss = 0.01051\n",
      "iteration =  272, loss = 0.01770\n",
      "iteration =  273, loss = 0.03317\n",
      "iteration =  274, loss = 0.01998\n",
      "iteration =  275, loss = 0.02341\n",
      "iteration =  276, loss = 0.03971\n",
      "iteration =  277, loss = 0.01606\n",
      "iteration =  278, loss = 0.00671\n",
      "iteration =  279, loss = 0.01239\n",
      "iteration =  280, loss = 0.04648\n",
      "iteration =  281, loss = 0.00294\n",
      "iteration =  282, loss = 0.02887\n",
      "iteration =  283, loss = 0.01511\n",
      "iteration =  284, loss = 0.01360\n",
      "iteration =  285, loss = 0.04320\n",
      "iteration =  286, loss = 0.02287\n",
      "iteration =  287, loss = 0.01573\n",
      "iteration =  288, loss = 0.01410\n",
      "iteration =  289, loss = 0.03530\n",
      "iteration =  290, loss = 0.01997\n",
      "iteration =  291, loss = 0.02460\n",
      "iteration =  292, loss = 0.02601\n",
      "iteration =  293, loss = 0.01943\n",
      "iteration =  294, loss = 0.01860\n",
      "iteration =  295, loss = 0.01512\n",
      "iteration =  296, loss = 0.02076\n",
      "iteration =  297, loss = 0.01183\n",
      "iteration =  298, loss = 0.00305\n",
      "iteration =  299, loss = 0.00597\n",
      "iteration =  300, loss = 0.00589\n",
      "iteration =  301, loss = 0.00329\n",
      "iteration =  302, loss = 0.00826\n",
      "iteration =  303, loss = 0.00806\n",
      "iteration =  304, loss = 0.00327\n",
      "iteration =  305, loss = 0.00287\n",
      "iteration =  306, loss = 0.00045\n",
      "iteration =  307, loss = 0.00220\n",
      "iteration =  308, loss = 0.00729\n",
      "iteration =  309, loss = 0.00417\n",
      "iteration =  310, loss = 0.00146\n",
      "iteration =  311, loss = 0.01679\n",
      "iteration =  312, loss = 0.00455\n",
      "iteration =  313, loss = 0.00270\n",
      "iteration =  314, loss = 0.00407\n",
      "iteration =  315, loss = 0.02245\n",
      "iteration =  316, loss = 0.00892\n",
      "iteration =  317, loss = 0.00943\n",
      "iteration =  318, loss = 0.00739\n",
      "iteration =  319, loss = 0.00228\n",
      "iteration =  320, loss = 0.00354\n",
      "iteration =  321, loss = 0.00416\n",
      "iteration =  322, loss = 0.00119\n",
      "iteration =  323, loss = 0.00265\n",
      "iteration =  324, loss = 0.00210\n",
      "iteration =  325, loss = 0.00221\n",
      "iteration =  326, loss = 0.00014\n",
      "iteration =  327, loss = 0.00325\n",
      "iteration =  328, loss = 0.00093\n",
      "iteration =  329, loss = 0.01182\n",
      "iteration =  330, loss = 0.00110\n",
      "iteration =  331, loss = 0.00080\n",
      "iteration =  332, loss = 0.00228\n",
      "iteration =  333, loss = 0.00352\n",
      "iteration =  334, loss = 0.00203\n",
      "iteration =  335, loss = 0.00668\n",
      "iteration =  336, loss = 0.00118\n",
      "iteration =  337, loss = 0.00069\n",
      "iteration =  338, loss = 0.00317\n",
      "iteration =  339, loss = 0.00419\n",
      "iteration =  340, loss = 0.00065\n",
      "iteration =  341, loss = 0.01053\n",
      "iteration =  342, loss = 0.00143\n",
      "iteration =  343, loss = 0.00013\n",
      "iteration =  344, loss = 0.00216\n",
      "iteration =  345, loss = 0.00434\n",
      "iteration =  346, loss = 0.00063\n",
      "iteration =  347, loss = 0.00128\n",
      "iteration =  348, loss = 0.00178\n",
      "iteration =  349, loss = 0.00071\n",
      "iteration =  350, loss = 0.00080\n",
      "iteration =  351, loss = 0.00024\n",
      "iteration =  352, loss = 0.00153\n",
      "iteration =  353, loss = 0.00078\n",
      "iteration =  354, loss = 0.00095\n",
      "iteration =  355, loss = 0.00044\n",
      "iteration =  356, loss = 0.00048\n",
      "iteration =  357, loss = 0.00006\n",
      "iteration =  358, loss = 0.00137\n",
      "iteration =  359, loss = 0.00063\n",
      "iteration =  360, loss = 0.00020\n",
      "iteration =  361, loss = 0.00181\n",
      "iteration =  362, loss = 0.00041\n",
      "iteration =  363, loss = 0.00118\n",
      "iteration =  364, loss = 0.00103\n",
      "iteration =  365, loss = 0.00066\n",
      "iteration =  366, loss = 0.00016\n",
      "iteration =  367, loss = 0.00144\n",
      "iteration =  368, loss = 0.00014\n",
      "iteration =  369, loss = 0.00190\n",
      "iteration =  370, loss = 0.00043\n",
      "iteration =  371, loss = 0.00041\n",
      "iteration =  372, loss = 0.00036\n",
      "iteration =  373, loss = 0.00074\n",
      "iteration =  374, loss = 0.00125\n",
      "iteration =  375, loss = 0.00048\n",
      "iteration =  376, loss = 0.00021\n",
      "iteration =  377, loss = 0.00080\n",
      "iteration =  378, loss = 0.00023\n",
      "iteration =  379, loss = 0.00020\n",
      "iteration =  380, loss = 0.00002\n",
      "iteration =  381, loss = 0.00040\n",
      "iteration =  382, loss = 0.00034\n",
      "iteration =  383, loss = 0.00345\n",
      "iteration =  384, loss = 0.00026\n",
      "iteration =  385, loss = 0.00025\n",
      "iteration =  386, loss = 0.00073\n",
      "iteration =  387, loss = 0.00042\n",
      "iteration =  388, loss = 0.00021\n",
      "iteration =  389, loss = 0.00023\n",
      "iteration =  390, loss = 0.00042\n",
      "iteration =  391, loss = 0.00482\n",
      "iteration =  392, loss = 0.00122\n",
      "iteration =  393, loss = 0.00086\n",
      "iteration =  394, loss = 0.00021\n",
      "iteration =  395, loss = 0.00081\n",
      "iteration =  396, loss = 0.00176\n",
      "iteration =  397, loss = 0.00038\n",
      "iteration =  398, loss = 0.00112\n",
      "iteration =  399, loss = 0.00322\n",
      "iteration =  400, loss = 0.00141\n",
      "iteration =  401, loss = 0.00054\n",
      "iteration =  402, loss = 0.00040\n",
      "iteration =  403, loss = 0.00296\n",
      "iteration =  404, loss = 0.00076\n",
      "iteration =  405, loss = 0.00164\n",
      "iteration =  406, loss = 0.00316\n",
      "iteration =  407, loss = 0.00162\n",
      "iteration =  408, loss = 0.00005\n",
      "iteration =  409, loss = 0.00069\n",
      "iteration =  410, loss = 0.00081\n",
      "iteration =  411, loss = 0.00073\n",
      "iteration =  412, loss = 0.00055\n",
      "iteration =  413, loss = 0.00246\n",
      "iteration =  414, loss = 0.00385\n",
      "iteration =  415, loss = 0.00616\n",
      "iteration =  416, loss = 0.00535\n",
      "iteration =  417, loss = 0.00026\n",
      "iteration =  418, loss = 0.00050\n",
      "iteration =  419, loss = 0.00517\n",
      "iteration =  420, loss = 0.00058\n",
      "iteration =  421, loss = 0.00072\n",
      "iteration =  422, loss = 0.00156\n",
      "iteration =  423, loss = 0.01109\n",
      "iteration =  424, loss = 0.00163\n",
      "iteration =  425, loss = 0.00122\n",
      "iteration =  426, loss = 0.00114\n",
      "iteration =  427, loss = 0.00653\n",
      "iteration =  428, loss = 0.00320\n",
      "iteration =  429, loss = 0.00711\n",
      "iteration =  430, loss = 0.00077\n",
      "iteration =  431, loss = 0.00391\n",
      "iteration =  432, loss = 0.00514\n",
      "iteration =  433, loss = 0.00184\n",
      "iteration =  434, loss = 0.00199\n",
      "iteration =  435, loss = 0.00318\n",
      "iteration =  436, loss = 0.00498\n",
      "iteration =  437, loss = 0.00258\n",
      "iteration =  438, loss = 0.00899\n",
      "iteration =  439, loss = 0.00250\n",
      "iteration =  440, loss = 0.00809\n",
      "iteration =  441, loss = 0.00344\n",
      "iteration =  442, loss = 0.00469\n",
      "iteration =  443, loss = 0.00170\n",
      "iteration =  444, loss = 0.00205\n",
      "iteration =  445, loss = 0.00532\n",
      "iteration =  446, loss = 0.00155\n",
      "iteration =  447, loss = 0.00280\n",
      "iteration =  448, loss = 0.00056\n",
      "iteration =  449, loss = 0.00405\n",
      "iteration =  450, loss = 0.00284\n",
      "iteration =  451, loss = 0.01337\n",
      "iteration =  452, loss = 0.00397\n",
      "iteration =  453, loss = 0.00346\n",
      "iteration =  454, loss = 0.00617\n",
      "iteration =  455, loss = 0.00318\n",
      "iteration =  456, loss = 0.00609\n",
      "iteration =  457, loss = 0.00123\n",
      "iteration =  458, loss = 0.00327\n",
      "iteration =  459, loss = 0.00443\n",
      "iteration =  460, loss = 0.00211\n",
      "iteration =  461, loss = 0.00598\n",
      "iteration =  462, loss = 0.00645\n",
      "iteration =  463, loss = 0.01079\n",
      "iteration =  464, loss = 0.01197\n",
      "iteration =  465, loss = 0.00440\n",
      "iteration =  466, loss = 0.01025\n",
      "iteration =  467, loss = 0.02744\n",
      "iteration =  468, loss = 0.00410\n",
      "iteration =  469, loss = 0.03270\n",
      "iteration =  470, loss = 0.00375\n",
      "iteration =  471, loss = 0.00627\n",
      "iteration =  472, loss = 0.00697\n",
      "iteration =  473, loss = 0.00906\n",
      "iteration =  474, loss = 0.00182\n",
      "iteration =  475, loss = 0.01250\n",
      "iteration =  476, loss = 0.00198\n",
      "iteration =  477, loss = 0.00573\n",
      "iteration =  478, loss = 0.00730\n",
      "iteration =  479, loss = 0.00259\n",
      "iteration =  480, loss = 0.00051\n",
      "iteration =  481, loss = 0.00645\n",
      "iteration =  482, loss = 0.01019\n",
      "iteration =  483, loss = 0.02153\n",
      "iteration =  484, loss = 0.01601\n",
      "iteration =  485, loss = 0.01529\n",
      "iteration =  486, loss = 0.00015\n",
      "iteration =  487, loss = 0.01036\n",
      "iteration =  488, loss = 0.00132\n",
      "iteration =  489, loss = 0.00994\n",
      "iteration =  490, loss = 0.02283\n",
      "iteration =  491, loss = 0.01501\n",
      "iteration =  492, loss = 0.01200\n",
      "iteration =  493, loss = 0.01130\n",
      "iteration =  494, loss = 0.00131\n",
      "iteration =  495, loss = 0.01233\n",
      "iteration =  496, loss = 0.00635\n",
      "iteration =  497, loss = 0.00702\n",
      "iteration =  498, loss = 0.00350\n",
      "iteration =  499, loss = 0.00662\n",
      "iteration =  500, loss = 5.00802\n",
      "iteration =  501, loss = 0.21041\n",
      "iteration =  502, loss = 0.10341\n",
      "iteration =  503, loss = 0.02190\n",
      "iteration =  504, loss = 0.04271\n",
      "iteration =  505, loss = 0.08594\n",
      "iteration =  506, loss = 0.05514\n",
      "iteration =  507, loss = 0.00248\n",
      "iteration =  508, loss = 0.02646\n",
      "iteration =  509, loss = 0.03388\n",
      "iteration =  510, loss = 0.03356\n",
      "iteration =  511, loss = 0.03551\n",
      "iteration =  512, loss = 0.00425\n",
      "iteration =  513, loss = 0.01216\n",
      "iteration =  514, loss = 0.07117\n",
      "iteration =  515, loss = 0.00960\n",
      "iteration =  516, loss = 0.00307\n",
      "iteration =  517, loss = 0.00757\n",
      "iteration =  518, loss = 0.00373\n",
      "iteration =  519, loss = 0.00149\n",
      "iteration =  520, loss = 0.03462\n",
      "iteration =  521, loss = 0.05198\n",
      "iteration =  522, loss = 0.00017\n",
      "iteration =  523, loss = 0.00058\n",
      "iteration =  524, loss = 0.03345\n",
      "iteration =  525, loss = 0.02178\n",
      "iteration =  526, loss = 0.06965\n",
      "iteration =  527, loss = 0.00032\n",
      "iteration =  528, loss = 0.00079\n",
      "iteration =  529, loss = 0.00047\n",
      "iteration =  530, loss = 0.00047\n",
      "iteration =  531, loss = 0.04340\n",
      "iteration =  532, loss = 0.00103\n",
      "iteration =  533, loss = 0.04493\n",
      "iteration =  534, loss = 0.00475\n",
      "iteration =  535, loss = 0.00690\n",
      "iteration =  536, loss = 0.02912\n",
      "iteration =  537, loss = 0.00101\n",
      "iteration =  538, loss = 0.00883\n",
      "iteration =  539, loss = 0.00487\n",
      "iteration =  540, loss = 0.00075\n",
      "iteration =  541, loss = 0.02044\n",
      "iteration =  542, loss = 0.00679\n",
      "iteration =  543, loss = 0.01706\n",
      "iteration =  544, loss = 0.01554\n",
      "iteration =  545, loss = 0.01846\n",
      "iteration =  546, loss = 0.01453\n",
      "iteration =  547, loss = 0.02467\n",
      "iteration =  548, loss = 0.00043\n",
      "iteration =  549, loss = 0.02082\n",
      "iteration =  550, loss = 0.00070\n",
      "iteration =  551, loss = 0.02287\n",
      "iteration =  552, loss = 0.00001\n",
      "iteration =  553, loss = 0.00024\n",
      "iteration =  554, loss = 0.00871\n",
      "iteration =  555, loss = 0.00001\n",
      "iteration =  556, loss = 0.00031\n",
      "iteration =  557, loss = 0.00023\n",
      "iteration =  558, loss = 0.00005\n",
      "iteration =  559, loss = 0.00040\n",
      "iteration =  560, loss = 0.00006\n",
      "iteration =  561, loss = 0.00252\n",
      "iteration =  562, loss = 0.00001\n",
      "iteration =  563, loss = 0.00033\n",
      "iteration =  564, loss = 0.00007\n",
      "iteration =  565, loss = 0.00349\n",
      "iteration =  566, loss = 0.00305\n",
      "iteration =  567, loss = 0.00780\n",
      "iteration =  568, loss = 0.00009\n",
      "iteration =  569, loss = 0.00333\n",
      "iteration =  570, loss = 0.00055\n",
      "iteration =  571, loss = 0.00486\n",
      "iteration =  572, loss = 0.00003\n",
      "iteration =  573, loss = 0.00004\n",
      "iteration =  574, loss = 0.00031\n",
      "iteration =  575, loss = 0.00277\n",
      "iteration =  576, loss = 0.00004\n",
      "iteration =  577, loss = 0.00672\n",
      "iteration =  578, loss = 0.00002\n",
      "iteration =  579, loss = 0.00062\n",
      "iteration =  580, loss = 0.00064\n",
      "iteration =  581, loss = 0.00161\n",
      "iteration =  582, loss = 0.00207\n",
      "iteration =  583, loss = 0.00434\n",
      "iteration =  584, loss = 0.00001\n",
      "iteration =  585, loss = 0.00297\n",
      "iteration =  586, loss = 0.00556\n",
      "iteration =  587, loss = 0.00018\n",
      "iteration =  588, loss = 0.00029\n",
      "iteration =  589, loss = 0.00511\n",
      "iteration =  590, loss = 0.00036\n",
      "iteration =  591, loss = 0.00235\n",
      "iteration =  592, loss = 0.00002\n",
      "iteration =  593, loss = 0.00001\n",
      "iteration =  594, loss = 0.00012\n",
      "iteration =  595, loss = 0.00009\n",
      "iteration =  596, loss = 0.00386\n",
      "iteration =  597, loss = 0.00098\n",
      "iteration =  598, loss = 0.00052\n",
      "iteration =  599, loss = 0.00143\n",
      "iteration =  600, loss = 0.00044\n",
      "iteration =  601, loss = 0.00500\n",
      "iteration =  602, loss = 0.00047\n",
      "iteration =  603, loss = 0.00018\n",
      "iteration =  604, loss = 0.00045\n",
      "iteration =  605, loss = 0.00602\n",
      "iteration =  606, loss = 0.00004\n",
      "iteration =  607, loss = 0.00014\n",
      "iteration =  608, loss = 0.00043\n",
      "iteration =  609, loss = 0.00002\n",
      "iteration =  610, loss = 0.00018\n",
      "iteration =  611, loss = 0.00034\n",
      "iteration =  612, loss = 0.00011\n",
      "iteration =  613, loss = 0.00004\n",
      "iteration =  614, loss = 0.00448\n",
      "iteration =  615, loss = 0.00045\n",
      "iteration =  616, loss = 0.00612\n",
      "iteration =  617, loss = 0.00021\n",
      "iteration =  618, loss = 0.00042\n",
      "iteration =  619, loss = 0.00077\n",
      "iteration =  620, loss = 0.00046\n",
      "iteration =  621, loss = 0.00334\n",
      "iteration =  622, loss = 0.00152\n",
      "iteration =  623, loss = 0.00082\n",
      "iteration =  624, loss = 0.00643\n",
      "iteration =  625, loss = 0.00184\n",
      "iteration =  626, loss = 0.00004\n",
      "iteration =  627, loss = 0.00017\n",
      "iteration =  628, loss = 0.00027\n",
      "iteration =  629, loss = 0.00018\n",
      "iteration =  630, loss = 0.00018\n",
      "iteration =  631, loss = 0.00123\n",
      "iteration =  632, loss = 0.00118\n",
      "iteration =  633, loss = 0.00018\n",
      "iteration =  634, loss = 0.00054\n",
      "iteration =  635, loss = 0.00013\n",
      "iteration =  636, loss = 0.00215\n",
      "iteration =  637, loss = 0.00499\n",
      "iteration =  638, loss = 0.00187\n",
      "iteration =  639, loss = 0.00174\n",
      "iteration =  640, loss = 0.00093\n",
      "iteration =  641, loss = 0.00307\n",
      "iteration =  642, loss = 0.00001\n",
      "iteration =  643, loss = 0.00012\n",
      "iteration =  644, loss = 0.00097\n",
      "iteration =  645, loss = 0.00130\n",
      "iteration =  646, loss = 0.00159\n",
      "iteration =  647, loss = 0.00052\n",
      "iteration =  648, loss = 0.00010\n",
      "iteration =  649, loss = 0.00150\n",
      "iteration =  650, loss = 0.00119\n",
      "iteration =  651, loss = 0.00679\n",
      "iteration =  652, loss = 0.00203\n",
      "iteration =  653, loss = 0.00247\n",
      "iteration =  654, loss = 0.00036\n",
      "iteration =  655, loss = 0.00187\n",
      "iteration =  656, loss = 0.00039\n",
      "iteration =  657, loss = 0.00046\n",
      "iteration =  658, loss = 0.00500\n",
      "iteration =  659, loss = 0.00424\n",
      "iteration =  660, loss = 0.00153\n",
      "iteration =  661, loss = 0.00077\n",
      "iteration =  662, loss = 0.00101\n",
      "iteration =  663, loss = 0.00097\n",
      "iteration =  664, loss = 0.00043\n",
      "iteration =  665, loss = 0.00224\n",
      "iteration =  666, loss = 0.00034\n",
      "iteration =  667, loss = 0.00355\n",
      "iteration =  668, loss = 0.00409\n",
      "iteration =  669, loss = 0.00062\n",
      "iteration =  670, loss = 0.00185\n",
      "iteration =  671, loss = 0.00066\n",
      "iteration =  672, loss = 0.00334\n",
      "iteration =  673, loss = 0.00076\n",
      "iteration =  674, loss = 0.00118\n",
      "iteration =  675, loss = 0.00356\n",
      "iteration =  676, loss = 0.00282\n",
      "iteration =  677, loss = 0.00174\n",
      "iteration =  678, loss = 0.00191\n",
      "iteration =  679, loss = 0.01200\n",
      "iteration =  680, loss = 0.00319\n",
      "iteration =  681, loss = 0.00204\n",
      "iteration =  682, loss = 0.00026\n",
      "iteration =  683, loss = 0.00009\n",
      "iteration =  684, loss = 0.00004\n",
      "iteration =  685, loss = 0.00167\n",
      "iteration =  686, loss = 0.00576\n",
      "iteration =  687, loss = 0.00662\n",
      "iteration =  688, loss = 0.00193\n",
      "iteration =  689, loss = 0.00894\n",
      "iteration =  690, loss = 0.00081\n",
      "iteration =  691, loss = 0.00302\n",
      "iteration =  692, loss = 0.00161\n",
      "iteration =  693, loss = 0.00353\n",
      "iteration =  694, loss = 0.00163\n",
      "iteration =  695, loss = 0.00987\n",
      "iteration =  696, loss = 0.00191\n",
      "iteration =  697, loss = 0.00108\n",
      "iteration =  698, loss = 0.00026\n",
      "iteration =  699, loss = 0.02123\n",
      "iteration =  700, loss = 0.00265\n",
      "iteration =  701, loss = 0.01033\n",
      "iteration =  702, loss = 0.00192\n",
      "iteration =  703, loss = 0.01974\n",
      "iteration =  704, loss = 0.00096\n",
      "iteration =  705, loss = 0.00228\n",
      "iteration =  706, loss = 0.00079\n",
      "iteration =  707, loss = 0.00351\n",
      "iteration =  708, loss = 0.00658\n",
      "iteration =  709, loss = 0.00620\n",
      "iteration =  710, loss = 0.01299\n",
      "iteration =  711, loss = 0.00071\n",
      "iteration =  712, loss = 0.04839\n",
      "iteration =  713, loss = 0.01476\n",
      "iteration =  714, loss = 0.00075\n",
      "iteration =  715, loss = 0.00069\n",
      "iteration =  716, loss = 0.00571\n",
      "iteration =  717, loss = 0.00062\n",
      "iteration =  718, loss = 0.00094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  719, loss = 0.00506\n",
      "iteration =  720, loss = 0.00667\n",
      "iteration =  721, loss = 0.00863\n",
      "iteration =  722, loss = 0.00070\n",
      "iteration =  723, loss = 0.00096\n",
      "iteration =  724, loss = 0.01498\n",
      "iteration =  725, loss = 0.00034\n",
      "iteration =  726, loss = 0.00218\n",
      "iteration =  727, loss = 0.01174\n",
      "iteration =  728, loss = 0.00135\n",
      "iteration =  729, loss = 0.00054\n",
      "iteration =  730, loss = 0.00122\n",
      "iteration =  731, loss = 0.00450\n",
      "iteration =  732, loss = 0.00419\n",
      "iteration =  733, loss = 0.00020\n",
      "iteration =  734, loss = 0.00898\n",
      "iteration =  735, loss = 0.00047\n",
      "iteration =  736, loss = 0.00239\n",
      "iteration =  737, loss = 0.00515\n",
      "iteration =  738, loss = 0.00176\n",
      "iteration =  739, loss = 0.00412\n",
      "iteration =  740, loss = 0.00016\n",
      "iteration =  741, loss = 0.00158\n",
      "iteration =  742, loss = 0.00128\n",
      "iteration =  743, loss = 0.00649\n",
      "iteration =  744, loss = 0.02063\n",
      "iteration =  745, loss = 0.00061\n",
      "iteration =  746, loss = 0.00035\n",
      "iteration =  747, loss = 0.00227\n",
      "iteration =  748, loss = 0.00581\n",
      "iteration =  749, loss = 0.01303\n",
      "iteration =  750, loss = 0.00069\n",
      "iteration =  751, loss = 0.00164\n",
      "iteration =  752, loss = 0.00103\n",
      "iteration =  753, loss = 0.00051\n",
      "iteration =  754, loss = 0.00429\n",
      "iteration =  755, loss = 0.00852\n",
      "iteration =  756, loss = 0.00138\n",
      "iteration =  757, loss = 0.00127\n",
      "iteration =  758, loss = 0.00007\n",
      "iteration =  759, loss = 0.00006\n",
      "iteration =  760, loss = 0.00011\n",
      "iteration =  761, loss = 0.00021\n",
      "iteration =  762, loss = 0.00122\n",
      "iteration =  763, loss = 0.00132\n",
      "iteration =  764, loss = 0.00592\n",
      "iteration =  765, loss = 0.00166\n",
      "iteration =  766, loss = 0.00000\n",
      "iteration =  767, loss = 0.00142\n",
      "iteration =  768, loss = 0.00063\n",
      "iteration =  769, loss = 0.00004\n",
      "iteration =  770, loss = 0.00003\n",
      "iteration =  771, loss = 0.00212\n",
      "iteration =  772, loss = 0.00040\n",
      "iteration =  773, loss = 0.00007\n",
      "iteration =  774, loss = 0.00001\n",
      "iteration =  775, loss = 0.00002\n",
      "iteration =  776, loss = 0.00002\n",
      "iteration =  777, loss = 0.00169\n",
      "iteration =  778, loss = 0.00000\n",
      "iteration =  779, loss = 0.00001\n",
      "iteration =  780, loss = 0.00003\n",
      "iteration =  781, loss = 0.00000\n",
      "iteration =  782, loss = 0.00033\n",
      "iteration =  783, loss = 0.00001\n",
      "iteration =  784, loss = 0.00001\n",
      "iteration =  785, loss = 0.00005\n",
      "iteration =  786, loss = 0.00002\n",
      "iteration =  787, loss = 0.00000\n",
      "iteration =  788, loss = 0.00010\n",
      "iteration =  789, loss = 0.00000\n",
      "iteration =  790, loss = 0.00004\n",
      "iteration =  791, loss = 0.00005\n",
      "iteration =  792, loss = 0.00004\n",
      "iteration =  793, loss = 0.00035\n",
      "iteration =  794, loss = 0.00130\n",
      "iteration =  795, loss = 0.00001\n",
      "iteration =  796, loss = 0.00000\n",
      "iteration =  797, loss = 0.00000\n",
      "iteration =  798, loss = 0.00008\n",
      "iteration =  799, loss = 0.00004\n",
      "iteration =  800, loss = 0.00000\n",
      "iteration =  801, loss = 0.00000\n",
      "iteration =  802, loss = 0.00000\n",
      "iteration =  803, loss = 0.00016\n",
      "iteration =  804, loss = 0.00000\n",
      "iteration =  805, loss = 0.00000\n",
      "iteration =  806, loss = 0.00000\n",
      "iteration =  807, loss = 0.00000\n",
      "iteration =  808, loss = 0.00000\n",
      "iteration =  809, loss = 0.00002\n",
      "iteration =  810, loss = 0.00001\n",
      "iteration =  811, loss = 0.00000\n",
      "iteration =  812, loss = 0.00001\n",
      "iteration =  813, loss = 0.00000\n",
      "iteration =  814, loss = 0.00001\n",
      "iteration =  815, loss = 0.00000\n",
      "iteration =  816, loss = 0.00000\n",
      "iteration =  817, loss = 0.00001\n",
      "iteration =  818, loss = 0.00000\n",
      "iteration =  819, loss = 0.00001\n",
      "iteration =  820, loss = 0.00000\n",
      "iteration =  821, loss = 0.00000\n",
      "iteration =  822, loss = 0.00000\n",
      "iteration =  823, loss = 0.00000\n",
      "iteration =  824, loss = 0.00000\n",
      "iteration =  825, loss = 0.00000\n",
      "iteration =  826, loss = 0.00000\n",
      "iteration =  827, loss = 0.00000\n",
      "iteration =  828, loss = 0.00000\n",
      "iteration =  829, loss = 0.00000\n",
      "iteration =  830, loss = 0.00000\n",
      "iteration =  831, loss = 0.00000\n",
      "iteration =  832, loss = 0.00000\n",
      "iteration =  833, loss = 0.00000\n",
      "iteration =  834, loss = 0.00000\n",
      "iteration =  835, loss = 0.00000\n",
      "iteration =  836, loss = 0.00000\n",
      "iteration =  837, loss = 0.00000\n",
      "iteration =  838, loss = 0.00000\n",
      "iteration =  839, loss = 0.00000\n",
      "iteration =  840, loss = 0.00000\n",
      "iteration =  841, loss = 0.00000\n",
      "iteration =  842, loss = 0.00000\n",
      "iteration =  843, loss = 0.00000\n",
      "iteration =  844, loss = 0.00000\n",
      "iteration =  845, loss = 0.00000\n",
      "iteration =  846, loss = 0.00000\n",
      "iteration =  847, loss = 0.00000\n",
      "iteration =  848, loss = 0.00000\n",
      "iteration =  849, loss = 0.00000\n",
      "iteration =  850, loss = 0.00000\n",
      "iteration =  851, loss = 0.00000\n",
      "iteration =  852, loss = 0.00000\n",
      "iteration =  853, loss = 0.00000\n",
      "iteration =  854, loss = 0.00000\n",
      "iteration =  855, loss = 0.00000\n",
      "iteration =  856, loss = 0.00000\n",
      "iteration =  857, loss = 0.00000\n",
      "iteration =  858, loss = 0.00000\n",
      "iteration =  859, loss = 0.00000\n",
      "iteration =  860, loss = 0.00000\n",
      "iteration =  861, loss = 0.00000\n",
      "iteration =  862, loss = 0.00000\n",
      "iteration =  863, loss = 0.00000\n",
      "iteration =  864, loss = 0.00000\n",
      "iteration =  865, loss = 0.00000\n",
      "iteration =  866, loss = 0.00000\n",
      "iteration =  867, loss = 0.00000\n",
      "iteration =  868, loss = 0.00000\n",
      "iteration =  869, loss = 0.00000\n",
      "iteration =  870, loss = 0.00000\n",
      "iteration =  871, loss = 0.00000\n",
      "iteration =  872, loss = 0.00000\n",
      "iteration =  873, loss = 0.00000\n",
      "iteration =  874, loss = 0.00000\n",
      "iteration =  875, loss = 0.00000\n",
      "iteration =  876, loss = 0.00000\n",
      "iteration =  877, loss = 0.00000\n",
      "iteration =  878, loss = 0.00000\n",
      "iteration =  879, loss = 0.00000\n",
      "iteration =  880, loss = 0.00000\n",
      "iteration =  881, loss = 0.00000\n",
      "iteration =  882, loss = 0.00000\n",
      "iteration =  883, loss = 0.00000\n",
      "iteration =  884, loss = 0.00001\n",
      "iteration =  885, loss = 0.00000\n",
      "iteration =  886, loss = 0.00000\n",
      "iteration =  887, loss = 0.00000\n",
      "iteration =  888, loss = 0.00000\n",
      "iteration =  889, loss = 0.00000\n",
      "iteration =  890, loss = 0.00000\n",
      "iteration =  891, loss = 0.00000\n",
      "iteration =  892, loss = 0.00000\n",
      "iteration =  893, loss = 0.00000\n",
      "iteration =  894, loss = 0.00000\n",
      "iteration =  895, loss = 0.00000\n",
      "iteration =  896, loss = 0.00000\n",
      "iteration =  897, loss = 0.00000\n",
      "iteration =  898, loss = 0.00000\n",
      "iteration =  899, loss = 0.00000\n",
      "iteration =  900, loss = 0.00000\n",
      "iteration =  901, loss = 0.00000\n",
      "iteration =  902, loss = 0.00000\n",
      "iteration =  903, loss = 0.00000\n",
      "iteration =  904, loss = 0.00000\n",
      "iteration =  905, loss = 0.00000\n",
      "iteration =  906, loss = 0.00000\n",
      "iteration =  907, loss = 0.00000\n",
      "iteration =  908, loss = 0.00000\n",
      "iteration =  909, loss = 0.00000\n",
      "iteration =  910, loss = 0.00000\n",
      "iteration =  911, loss = 0.00000\n",
      "iteration =  912, loss = 0.00000\n",
      "iteration =  913, loss = 0.00000\n",
      "iteration =  914, loss = 0.00000\n",
      "iteration =  915, loss = 0.00000\n",
      "iteration =  916, loss = 0.00000\n",
      "iteration =  917, loss = 0.00000\n",
      "iteration =  918, loss = 0.00000\n",
      "iteration =  919, loss = 0.00000\n",
      "iteration =  920, loss = 0.00000\n",
      "iteration =  921, loss = 0.00000\n",
      "iteration =  922, loss = 0.00000\n",
      "iteration =  923, loss = 0.00001\n",
      "iteration =  924, loss = 0.00000\n",
      "iteration =  925, loss = 0.00000\n",
      "iteration =  926, loss = 0.00001\n",
      "iteration =  927, loss = 0.00000\n",
      "iteration =  928, loss = 0.00000\n",
      "iteration =  929, loss = 0.00000\n",
      "iteration =  930, loss = 0.00000\n",
      "iteration =  931, loss = 0.00000\n",
      "iteration =  932, loss = 0.00001\n",
      "iteration =  933, loss = 0.00000\n",
      "iteration =  934, loss = 0.00001\n",
      "iteration =  935, loss = 0.00000\n",
      "iteration =  936, loss = 0.00000\n",
      "iteration =  937, loss = 0.00001\n",
      "iteration =  938, loss = 0.00000\n",
      "iteration =  939, loss = 0.00000\n",
      "iteration =  940, loss = 0.00000\n",
      "iteration =  941, loss = 0.00001\n",
      "iteration =  942, loss = 0.00000\n",
      "iteration =  943, loss = 0.00000\n",
      "iteration =  944, loss = 0.00000\n",
      "iteration =  945, loss = 0.00001\n",
      "iteration =  946, loss = 0.00000\n",
      "iteration =  947, loss = 0.00002\n",
      "iteration =  948, loss = 0.00000\n",
      "iteration =  949, loss = 0.00003\n",
      "iteration =  950, loss = 0.00000\n",
      "iteration =  951, loss = 0.00000\n",
      "iteration =  952, loss = 0.00000\n",
      "iteration =  953, loss = 0.00000\n",
      "iteration =  954, loss = 0.00000\n",
      "iteration =  955, loss = 0.00001\n",
      "iteration =  956, loss = 0.00006\n",
      "iteration =  957, loss = 0.00023\n",
      "iteration =  958, loss = 0.00000\n",
      "iteration =  959, loss = 0.00001\n",
      "iteration =  960, loss = 0.00001\n",
      "iteration =  961, loss = 0.00001\n",
      "iteration =  962, loss = 0.00001\n",
      "iteration =  963, loss = 0.00001\n",
      "iteration =  964, loss = 0.00001\n",
      "iteration =  965, loss = 0.00003\n",
      "iteration =  966, loss = 0.00030\n",
      "iteration =  967, loss = 0.00004\n",
      "iteration =  968, loss = 0.00003\n",
      "iteration =  969, loss = 0.00000\n",
      "iteration =  970, loss = 0.00008\n",
      "iteration =  971, loss = 0.00000\n",
      "iteration =  972, loss = 0.00000\n",
      "iteration =  973, loss = 0.00001\n",
      "iteration =  974, loss = 0.00000\n",
      "iteration =  975, loss = 0.00001\n",
      "iteration =  976, loss = 0.00001\n",
      "iteration =  977, loss = 0.00000\n",
      "iteration =  978, loss = 0.00001\n",
      "iteration =  979, loss = 0.00000\n",
      "iteration =  980, loss = 0.00000\n",
      "iteration =  981, loss = 0.00006\n",
      "iteration =  982, loss = 0.00000\n",
      "iteration =  983, loss = 0.00003\n",
      "iteration =  984, loss = 0.00050\n",
      "iteration =  985, loss = 0.00000\n",
      "iteration =  986, loss = 0.00000\n",
      "iteration =  987, loss = 0.00000\n",
      "iteration =  988, loss = 0.00001\n",
      "iteration =  989, loss = 0.00001\n",
      "iteration =  990, loss = 0.00000\n",
      "iteration =  991, loss = 0.00007\n",
      "iteration =  992, loss = 0.00000\n",
      "iteration =  993, loss = 0.00000\n",
      "iteration =  994, loss = 0.00000\n",
      "iteration =  995, loss = 0.00000\n",
      "iteration =  996, loss = 0.00000\n",
      "iteration =  997, loss = 0.00000\n",
      "iteration =  998, loss = 0.00000\n",
      "iteration =  999, loss = 0.00001\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6eb30fde3f4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtheta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcompute_gradient_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_function1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_function1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtheta1_iteration\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss1_iteration\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 0 with size 1000"
     ]
    }
   ],
   "source": [
    "for i in range(num_iteration):\n",
    "    theta1 = theta1 - learning_rate * compute_gradient_feature(theta1, feature_function1(x1[i], y1[i]), label1[i])\n",
    "    loss1 = compute_loss_feature(theta1, feature_function1(x1[i], y1[i]), label1[i])\n",
    "    theta1_iteration[i] = theta1\n",
    "    loss1_iteration[i] = loss1\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss1))\n",
    "print(\"###########################################################\")\n",
    "for i in range(num_iteration):\n",
    "    theta2 = theta2 - learning_rate * compute_gradient_feature(theta2, feature_function2(x2[i], y2[i]), label2[i])\n",
    "    loss2 = compute_loss_feature(theta2, feature_function2(x2[i], y2[i]), label2[i])\n",
    "    theta2_iteration[i] = theta2\n",
    "    loss2_iteration[i] = loss2\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss2))\n",
    "    \n",
    "theta1_optimal = theta1\n",
    "theta2_optimal = theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute accuracy of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = np.array([feature_function1(x1[i], y1[i]) for i in range(0, len(x1))])\n",
    "feature2 = np.array([feature_function2(x2[i], y2[i]) for i in range(0, len(x2))])\n",
    "accuracy_classifier1 = compute_accuracy(theta1_optimal, feature1, label1)\n",
    "accuracy_classifier2 = compute_accuracy(theta2_optimal, feature2, label2)\n",
    "print(accuracy_classifier1)\n",
    "print(accuracy_classifier2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))   \n",
    "    plt.title('loss')\n",
    "\n",
    "    plt.plot(loss_iteration, '-', color = 'red')\n",
    "\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(point_x, point_y, label):\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.title('data')\n",
    "    xx = []\n",
    "    yy = []\n",
    "    xxx = []\n",
    "    yyy = []\n",
    "    for i in range(0, num_data1) :\n",
    "        x = point_x[i]\n",
    "        y = point_y[i]\n",
    "        if label[i] == 0 :\n",
    "            xx.append(x)\n",
    "            yy.append(y)\n",
    "        elif label[i] == 1 :\n",
    "            xxx.append(x)\n",
    "            yyy.append(y)\n",
    "    plt.scatter(xx,yy,c='blue', label = 'Class = 0')\n",
    "    plt.scatter(xxx,yyy,c='red', label = 'Class = 1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_parameter(theta_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('model parameter')\n",
    "    thetaT = theta_iteration.T\n",
    "    plt.plot(thetaT[0], '-', color = 'red', label = 'theta0')\n",
    "    plt.plot(thetaT[1], '-', color = 'green', label = 'theta1')\n",
    "    plt.plot(thetaT[2], '-', color = 'blue', label = 'theta2')\n",
    "    plt.plot(thetaT[3], '-', color = 'black', label = 'theta3')\n",
    "    if len(thetaT) > 4 :\n",
    "        plt.plot(thetaT[4], '-', color = 'orange', label = 'theta4')\n",
    "        plt.plot(thetaT[5], '-', color = 'skyblue', label = 'theta5')\n",
    "\n",
    "    plt.xlabel('iteration')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss_curve(loss1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss2_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta1_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta2_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier1(data, theta):\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    xx = []\n",
    "    yy = []\n",
    "    xxx = []\n",
    "    yyy = []\n",
    "    X = np.linspace(x1.min(), x1.max(), 1000)\n",
    "    Y = np.linspace(y1.min(), y1.max(), 1000)\n",
    "    gX, gY = np.meshgrid(X, Y)\n",
    "    gX = gX[0]\n",
    "    gY = gY[0]\n",
    "    Z = regression_function(theta1, feature_function1(gX, gY))\n",
    "\n",
    "    for i in range(0, num_data1) :\n",
    "        x = x1[i]\n",
    "        y = y1[i]\n",
    "        if label1[i] == 0 :\n",
    "            xx.append(x)\n",
    "            yy.append(y)\n",
    "        elif label1[i] == 1 :\n",
    "            xxx.append(x)\n",
    "            yyy.append(y)\n",
    "    print(Z.shape)\n",
    "    plt.scatter(xx, yy, color='b', label='Class = 0')\n",
    "    plt.scatter(xxx, yyy, color='r', label='Class = 1')\n",
    "    plt.contourf(gX, gY, Z, levels=100, cmap = 'bwr')\n",
    "    plt.colorbar()\n",
    "    plt.contour(gX,gY, Z, levels = [0], colors = 'black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier2(data, theta):\n",
    "    \n",
    "    plt.figure(figsize=(8,8)) # USE THIS VALUE for the size of the figure\n",
    "    #\n",
    "    # \n",
    "    # fill up the function body\n",
    "    #\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier1(data1, theta1_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(data2, theta2_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 01. plot the input data (data1) from the file [assignment_09_data1.txt] in blue for class 0 and in red for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x1,y1,label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 02. plot the input data (data2) from the file [assignment_09_data2.txt] in blue for class 0 and in red for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x2,y2,label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 03. plot the values of the model parameters $\\theta$ as curves over the gradient descent iterations using different colors for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 04. plot the values of the model parameters $\\theta$ as curves over the gradient descent iterations using different colors for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta2_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 05. plot the loss values in red curve over the gradient descent iterations for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 06. plot the loss values in red curve over the gradient descent iterations for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss2_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 07. plot the classifier with the given data points superimposed for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier1(data1, theta1_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 08. plot the classifier with the given data points superimposed for data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(data2, theta2_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 09. print out the accuracy of the obtained classifier1 for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_classifier1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 10. print out the accuracy of the obtained classifier2 for data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_classifier2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
